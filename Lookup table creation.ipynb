{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing an interpolating lookup table, so we can simply look up the best length scale, given two points $D_1 = (x_1, y_1)$ and $D_2 = (x_2, y_2)$. The lookup table $f$ is constructed such that what we look up is $\\log(ell) = f(n, s),$ where $n$ indicates normalized measurement noise level and $s$ indicates normalized signal level. The normalization is such that $D_1 = (0,0)$ and $D_2 = (1,1)$. \n",
    "\n",
    "We compute $n$ and $s$ from $\\sigma_n$ and $\\sigma_s$. We fix $\\sigma_n$ by what we know about the lab test measurement noise (standard deviation of the measurement error), and we estimate $\\sigma_s$ as the standard deviation of the signal, by taking the standard devation of a quick smooth fit to the measurements. That won't be a perfect estimate, but it's the best we have. \n",
    "\n",
    "If  $\\Delta y = y_2 - y_1$, $\\Delta x = x_2 - x_1$, then we \n",
    "have $n = \\log(\\sigma_n / \\Delta y)$ and $s = \\log(\\sigma_s / \\Delta y).$\n",
    "\n",
    "Then we compute \n",
    "\\begin{align}\n",
    "    ell &= \\exp(f(n, s)) \\Delta x \\\\\n",
    "        &= \\exp(f(\\log(\\sigma_n / \\Delta y), \\log(\\sigma_s / \\Delta y)) \\Delta x.\n",
    "\\end{align}  \n",
    "\n",
    "One remaining question: Why is the average loss (`losses`) of the 100 runs at a given set of parameters different from the recomputed loss (`gp_losses`) at those same parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "from timeit import default_timer as timer\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.interpolate import PchipInterpolator, RectBivariateSpline\n",
    "from datetime import datetime\n",
    "from warnings import simplefilter, warn, catch_warnings, filterwarnings\n",
    "import logging\n",
    "import pickle\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from gpytorch.utils.warnings import GPInputWarning\n",
    "# %matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_inputs,\n",
    "        train_targets,\n",
    "        ell_prior=gpytorch.priors.GammaPrior(concentration=2.0, rate=1.0),\n",
    "        likelihood=gpytorch.likelihoods.GaussianLikelihood(),\n",
    "        seed=None\n",
    "    ):\n",
    "        \"\"\"Create a stationary GP model\n",
    "\n",
    "        Args:\n",
    "            train_inputs (float): observation locations\n",
    "            train_targets (float): observation values\n",
    "            ell_prior(gpytorch prior): Defaults to GammaPrior(concentration=2.0, rate=0.1)\n",
    "            likelihood(gpytorch likelihood): Defaults to GaussianLikelihood().\n",
    "        \"\"\"\n",
    "        train_inputs = torch.as_tensor(train_inputs)\n",
    "        train_targets = torch.as_tensor(train_targets)\n",
    "        self.rng = default_rng(seed)\n",
    "        super().__init__(train_inputs, train_targets, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(lengthscale_prior=ell_prior))\n",
    "        self.likelihood = likelihood\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def lengthscale(self):\n",
    "        return self.covar_module.base_kernel.lengthscale.item()\n",
    "\n",
    "    @lengthscale.setter\n",
    "    def lengthscale(self, value):\n",
    "        self.covar_module.base_kernel.lengthscale = torch.tensor([value])\n",
    "\n",
    "    @property\n",
    "    def noise_var(self):\n",
    "        return self.likelihood.noise.item()\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self.best_loss.item()\n",
    "\n",
    "    @property\n",
    "    def signal_var(self):\n",
    "        return self.covar_module.outputscale.item()\n",
    "\n",
    "    @property\n",
    "    def offset(self):\n",
    "        return self.mean_module.constant.item()\n",
    "        \n",
    "    def loss_on(self, x, y):\n",
    "        self.eval()\n",
    "        self.likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self)\n",
    "            output = self.__call__(torch.as_tensor(x))\n",
    "            ll = -mll(output, torch.as_tensor(y))\n",
    "            return ll\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def fit(self, *, n_iter=100,  progress_threshold=100, tol=0.0, lr=0.1, signal_var=None, noise_var=None, offset=None, lengthscale=None, verbose=False):\n",
    "        # The initializations are specified with a two-point training set \n",
    "        # (0,0), (1,1) in mind.\n",
    "\n",
    "        if offset is not None:\n",
    "            self.mean_module.constant = offset \n",
    "            self.mean_module.raw_constant.requires_grad = False\n",
    "        else:\n",
    "            self.mean_module.constant = self.rng.uniform(-10.0, 10.0)\n",
    "            self.mean_module.raw_constant.requires_grad = True\n",
    "        \n",
    "        if lengthscale is not None:\n",
    "            self.covar_module.base_kernel.lengthscale = lengthscale\n",
    "            self.covar_module.base_kernel.raw_lengthscale.requires_grad = False\n",
    "        else:\n",
    "            self.covar_module.base_kernel.lengthscale = self.rng.uniform(1e-2,10.0)\n",
    "            self.covar_module.base_kernel.raw_lengthscale.requires_grad = True\n",
    "\n",
    "        if signal_var is not None:\n",
    "            self.covar_module.outputscale = signal_var \n",
    "            self.covar_module.raw_outputscale.requires_grad = False\n",
    "        else:\n",
    "            self.covar_module.outputscale = self.rng.uniform(1e-2, 10)\n",
    "            self.covar_module.raw_outputscale.requires_grad = True\n",
    "\n",
    "        if noise_var is not None:\n",
    "            self.likelihood.noise = noise_var\n",
    "            self.likelihood.raw_noise.requires_grad = False\n",
    "        else:\n",
    "            # mean: ln(0.1) = -2.3\n",
    "            self.likelihood.noise = self.rng.lognormal(mean=-2.3, sigma=1.0)\n",
    "            self.likelihood.raw_noise.requires_grad = True\n",
    "\n",
    "        losses = torch.FloatTensor(n_iter)\n",
    "        lengthscales = torch.FloatTensor(n_iter)\n",
    "        noise_vars = torch.FloatTensor(n_iter)\n",
    "        signal_vars = torch.FloatTensor(n_iter)\n",
    "        self.best_loss = torch.inf\n",
    "        best_index = None\n",
    "        last_loss = torch.inf\n",
    "        self.best_lengthscale = None\n",
    "        self.train()\n",
    "        self.likelihood.train()\n",
    "\n",
    "        params = self.parameters()\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "        # \"Loss\" for GPs - the marginal log likelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self)\n",
    "        n_no_improvement = 0\n",
    "        n_small_improvement = 0\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            \n",
    "\n",
    "            # Zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Output from model\n",
    "            # TODO: figure out how to work with a tuple of inputs, if needed.\n",
    "            output = self.__call__(self.train_inputs[0])\n",
    "            # Calc loss and backprop gradients\n",
    "            loss = -mll(output, self.train_targets)\n",
    "            loss.backward()\n",
    "\n",
    "            losses[i] = loss\n",
    "            lengthscales[i] = self.lengthscale\n",
    "            noise_vars[i] = self.noise_var\n",
    "            signal_vars[i] = self.signal_var\n",
    "\n",
    "            best_flag = \"\"\n",
    "            n_no_improvement += 1\n",
    "            n_small_improvement += 1\n",
    "            if loss < self.best_loss:\n",
    "                n_no_improvement = 0\n",
    "                best_flag = \"*\"\n",
    "                self.best_loss = loss\n",
    "                best_model = deepcopy(self.state_dict())\n",
    "                best_index = i\n",
    "                self.best_lengthscale = self.lengthscale\n",
    "            \n",
    "            if verbose:\n",
    "                ln_string = f\"{self.lengthscale:.4f}\"\n",
    "                print(\n",
    "                    f\"Iter {i:04d}/{n_iter} - Loss: {loss.item():.4f}  Lengthscale: {ln_string} noise: {self.noise_var:.3f} ({best_index:04d} {ln_string}){best_flag}\"\n",
    "                )\n",
    "\n",
    "            if n_no_improvement > progress_threshold:\n",
    "                if verbose:\n",
    "                    print( f\"Stopped after {n_no_improvement} iterations without improvement.\")\n",
    "                break\n",
    "\n",
    "            if last_loss - loss > tol:\n",
    "                n_small_improvement = 0\n",
    "                \n",
    "            if n_small_improvement > progress_threshold:\n",
    "                if verbose:\n",
    "                    print( f\"Stopped after {n_small_improvement} iterations with only small improvement.\")\n",
    "                break\n",
    "            last_loss = loss\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "        self.load_state_dict(best_model)\n",
    "        j = i+1\n",
    "        return losses.detach().numpy()[:j], noise_vars.detach().numpy()[:j], lengthscales.detach().numpy()[:j], signal_vars.detach().numpy()[:j], best_index\n",
    "\n",
    "    def predict(self, test_inputs):\n",
    "        self.eval()\n",
    "        self.likelihood.eval()\n",
    "        test_inputs = torch.as_tensor(test_inputs)\n",
    "        # Make predictions by feeding model through likelihood\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            preds = self.likelihood(self.__call__(test_inputs))\n",
    "\n",
    "        p_mean = preds.mean.numpy()\n",
    "        p_lower, p_upper = [x.numpy() for x in preds.confidence_region()]\n",
    "        return p_mean, p_lower, p_upper\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"GPModel Loss: {self.loss:0.10f}, ell: {self.lengthscale:0.4f}, svar: {self.signal_var}, nvar: {self.noise_var:0.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(gp, x, y, n):\n",
    "    pad = (x[-1] - x[0]) * 0.1\n",
    "    pred_x = np.linspace(x[0] - pad, x[-1] + pad, num=n)\n",
    "    pred_y, lower, upper = gp.predict(pred_x)\n",
    "    return pred_x, pred_y, lower, upper\n",
    "\n",
    "def plot_preds(gp, x, y, n=100, noise_var=None, signal_var=None, lengthscale=None):\n",
    "    pred_x, pred_y, lower, upper = get_preds(gp, x, y, n=n)\n",
    "    plt.plot(pred_x, pred_y, color='b')\n",
    "    plt.plot(x, y, 'ob')\n",
    "    plt.fill_between(pred_x, lower, upper, color='b', alpha=0.1)\n",
    "    plt.axhline(gp.offset, color='green', alpha=0.1)\n",
    "    barwidth = 4\n",
    "    if noise_var is not None:\n",
    "        plt.errorbar(x, y, yerr=np.sqrt(noise_var), linewidth=0, elinewidth=barwidth, ecolor='blue', alpha=0.5)\n",
    "    if signal_var is not None:\n",
    "        plt.plot((x[0], x[0]), (gp.offset, gp.offset+np.sqrt(signal_var)), '-b', linewidth=barwidth, alpha=0.5)\n",
    "    if lengthscale is not None:\n",
    "        plt.plot((x[0], x[0]+lengthscale), (gp.offset, gp.offset), '-b',linewidth=barwidth, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(losses, noise_vars, lengthscales, signal_vars, best_iter):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.semilogy(losses, 'b-', label=f'loss ({losses[best_iter]:0.4g})')\n",
    "    # plt.semilogy(noise_vars, label=f'noise var ({noise_vars[best_iter]:0.2g})')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.semilogy(lengthscales, 'g-', label=f'ell ({lengthscales[best_iter]:0.2g})')\n",
    "    # plt.semilogy(signal_vars, label=f'signal var ({signal_vars[best_iter]:0.2g})')\n",
    "    ax2.axvline(best_iter)\n",
    "    ax1.set_xlabel('iteration')\n",
    "    ax1.set_ylabel('Loss', color='b')\n",
    "    ax2.set_ylabel('lengthscale', color='g')\n",
    "    fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gp_once(x, y, *, verbose, n_iter, **kwargs):\n",
    "    gp = GPModel(x, y, ell_prior=None)\n",
    "    losses, nvs, ells, svs, best_iter = gp.fit(verbose=False, n_iter=n_iter, **kwargs) \n",
    "    if verbose:\n",
    "        print(f\"Iter {best_iter}/ {len(losses)} / {n_iter}\")\n",
    "    if best_iter == n_iter - 1:\n",
    "        logging.warning(f\"Fit reached iteration limit: {gp}\")\n",
    "        # plot_training(losses=losses, noise_vars=nvs, lengthscales=ells, signal_vars=svs, best_iter=best_iter)\n",
    "    return gp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gp_sampled(x, y,  *, verbose, n_samples, **kwargs ):\n",
    "    losses = np.empty(n_samples)\n",
    "    ells = np.empty(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        gp = fit_gp_once(x, y, verbose=verbose, **kwargs)\n",
    "        losses[i] = gp.loss\n",
    "        ells[i] = gp.lengthscale\n",
    "        if verbose:\n",
    "            print(f\"[{i:03d}]: {gp}\")\n",
    "    ell_mean = np.average(ells, weights=np.exp(-losses))\n",
    "    gp.lengthscale = ell_mean\n",
    "    \n",
    "    return gp, ell_mean, ells, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=50\n",
    "n_samples = 100\n",
    "logging.basicConfig(filename=f'sampled_table_creation_rep_{n_samples}_res_{res}_v2.log', encoding='utf-8', filemode='w', level=logging.INFO)\n",
    "filterwarnings(\"ignore\", category=GPInputWarning)\n",
    "\n",
    "x = np.array([0.0, 1.0])\n",
    "y = np.array([0.0, 1.0])\n",
    "offset = y.mean()\n",
    "\n",
    "n_iter = 1000\n",
    "tol = 1e-8\n",
    "progress_threshold = 100\n",
    "lr = 0.2\n",
    "\n",
    "signal_sds = np.logspace(-1, 1, num=res)\n",
    "slen = len(signal_sds)\n",
    "noise_sds = np.logspace(-1, 1, num=res)\n",
    "nlen = len(noise_sds)\n",
    "ells = np.full((slen, nlen), None)\n",
    "losses = np.full((slen, nlen), None)\n",
    "gp_losses = np.full((slen, nlen), None)\n",
    "for si in tqdm(range(slen), desc=\"signal\"):\n",
    "    for ni in tqdm(range(nlen), desc=\"noise\", leave=False):\n",
    "        sv = signal_sds[si]**2\n",
    "        nv = noise_sds[ni]**2\n",
    "        gp, ell_mean, iter_ells, iter_losses = fit_gp_sampled(x=x, y=y, offset=offset, signal_var=sv, noise_var=nv, n_samples=n_samples, n_iter=n_iter, progress_threshold=progress_threshold, tol=tol, lr=lr, verbose=False)    \n",
    "        ells[si, ni] = ell_mean\n",
    "        lm = iter_losses.mean()\n",
    "        losses[si, ni] = lm\n",
    "        lo = gp.loss_on(x, y)\n",
    "        gp_losses[si, ni] = lo\n",
    "        logging.info(f\"{datetime.now()} sv: {sv:0.2g}, nv:{nv:0.2g}, ell: {gp.lengthscale:0.3g} mean loss: {lm:0.5g} rerun loss: {lo:0.5g}\")\n",
    "\n",
    "with open(f'table_creation_rep_{n_samples}_res_{res}_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(ells, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(losses, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(gp_losses, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(signal_sds, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(noise_sds, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'table_creation_rep_{n_samples}_res_{res}_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(ells, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(losses, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(gp_losses, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(signal_sds, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(noise_sds, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(noise_sds)):\n",
    "    plt.loglog(noise_sds, ells[k,:], '-')\n",
    "    plt.xlabel('Measurement Noise SD')\n",
    "    plt.ylabel('Length Scale')\n",
    "plt.figure()\n",
    "for k in range(len(signal_sds)):\n",
    "    plt.loglog(signal_sds, ells[:,k], '-')\n",
    "    plt.xlabel('Signal SD')\n",
    "    plt.ylabel('Length Scale')\n",
    "plt.figure()\n",
    "for k in range(len(noise_sds)):\n",
    "    plt.loglog(noise_sds, losses[k,:], '-')\n",
    "    plt.xlabel('Measurement Noise SD')\n",
    "    plt.ylabel('Average Loss')\n",
    "plt.figure()\n",
    "for k in range(len(signal_sds)):\n",
    "    plt.loglog(signal_sds, losses[:,k], '-')\n",
    "    plt.xlabel('Signal SD')\n",
    "    plt.ylabel('Average Loss')\n",
    "plt.figure()\n",
    "for k in range(len(noise_sds)):\n",
    "    plt.loglog(noise_sds, gp_losses[k,:], '-')\n",
    "    plt.xlabel('Measurement Noise SD')\n",
    "    plt.ylabel('Recomputed Loss')\n",
    "plt.figure()\n",
    "for k in range(len(signal_sds)):\n",
    "    plt.loglog(signal_sds, gp_losses[:,k], '-')\n",
    "    plt.xlabel('Signal SD')\n",
    "    plt.ylabel('Recomputed Loss')\n",
    "for k in range(len(signal_sds)):\n",
    "    plt.loglog(losses[:,k], gp_losses[:,k], '-')\n",
    "    plt.xlabel('Average Loss')\n",
    "    plt.ylabel('Recomputed Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "X, Y = np.meshgrid(np.log10(noise_sds), np.log10(signal_sds))\n",
    "# X, Y = np.meshgrid(noise_sds, signal_sds)\n",
    "Z = np.log10(ells.astype(float))\n",
    "ax.plot_surface(X, Y, Z, cmap=\"autumn_r\", lw=0.5, rstride=1, cstride=1, alpha=0.5)\n",
    "# ax.contour(X, Y, Z, 10, lw=3, cmap=\"autumn_r\", linestyles=\"solid\", offset=-5)\n",
    "ax.contour(X, Y, Z, 10, lw=3, colors=\"k\", linestyles=\"solid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "x, y = np.log10(noise_sds), np.log10(signal_sds)\n",
    "Z = np.log10(ells.astype(float))\n",
    "interp_func = interpolate.interp2d(x, y, Z, kind='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../results/lookup_{n_samples}_res_{res}_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(interp_func, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a RectBivariateSpline version, because interp2d doesn't handle vectorized input for lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=50\n",
    "n_samples = 100\n",
    "\n",
    "x = np.array([0.0, 1.0])\n",
    "y = np.array([0.0, 1.0])\n",
    "offset = y.mean()\n",
    "\n",
    "n_iter = 1000\n",
    "tol = 1e-8\n",
    "progress_threshold = 100\n",
    "lr = 0.2\n",
    "\n",
    "signal_sds = np.logspace(-1, 1, num=res)\n",
    "slen = len(signal_sds)\n",
    "noise_sds = np.logspace(-1, 1, num=res)\n",
    "nlen = len(noise_sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'table_creation_rep_100_res_50_v2.pkl', 'rb') as f:\n",
    "    ells = pickle.load(f)\n",
    "    losses = pickle.load(f)\n",
    "    gp_losses = pickle.load(f)\n",
    "    signal_sds = pickle.load(f)\n",
    "    noise_sds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.log10(noise_sds), np.log10(signal_sds)\n",
    "Z = np.log10(ells.astype(float))\n",
    "interp_func = RectBivariateSpline(x, y, Z, kx=1, ky=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "xnew = np.arange(-1.00, 1.00, 1e-2)\n",
    "\n",
    "ynew = np.arange(-1.00, 1.00, 1e-2)\n",
    "\n",
    "znew = interp_func(xnew, ynew)\n",
    "i = 30\n",
    "plt.plot(x, Z[i, :], 'ro')\n",
    "plt.plot(xnew, znew[i * 4, :], 'bo-', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_reconstructed = interp_func(x, y)\n",
    "z_delta = Z - z_reconstructed\n",
    "\n",
    "(np.all(z_delta == 0), np.all(z_reconstructed == Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "znew = interp_func(xnew, ynew, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(znew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../results/rbs_lookup_{n_samples}_res_{res}.pkl', 'wb') as f:\n",
    "    pickle.dump(interp_func, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff5a0f34ec129f9c13dc8a34bc755b96233fd086b9b14da6bd37bcbf4bd59afe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
